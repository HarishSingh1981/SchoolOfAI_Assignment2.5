{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm33vEIS0LKNhOEzloec+B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarishSingh1981/SchoolOfAI_Assignment2.5/blob/main/Session2_5_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V21CgIQLVE0"
      },
      "outputs": [],
      "source": [
        "# Two kind of inputs to the network \n",
        "#1. MNist dataset\n",
        "#2. Random number\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.image_dataset = torchvision.datasets.MNIST(root =\"./data\",\n",
        "                                              train=True,\n",
        "                                              download=True,\n",
        "                                              transform=transforms.Compose([transforms.ToTensor()]))\n",
        "    print(f'Length of Mnist dataset -- > {len(self.image_dataset)}')\n",
        "  def __len__(self):\n",
        "    return len(self.image_dataset)\n",
        "  def __getitem__(self,index):\n",
        "    #get image and label touple from data set + random number and summation of image label\n",
        "    random_no = np.random.randint(0,9)\n",
        "    sum_rand_label = random_no + self.image_dataset[index][1]\n",
        "   # print(f'label for image data is {self.image_dataset[index][1]} and random number --> {random_no}')\n",
        "    #touple2 = (random_no,sum_rand_label)\n",
        "    rand_input = torch.tensor(np.zeros(10,dtype=np.float32),device= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    rand_sum = torch.tensor(np.zeros(19,dtype=np.float32),device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    rand_input[random_no] = 1\n",
        "    rand_sum[sum_rand_label] = 1\n",
        "    rand_input.requires_grad = True\n",
        "    rand_sum.requires_grad = True\n",
        "    #print(f'Random input {random_no} --> {rand_input}')\n",
        "    #print(f'Random sum {sum_rand_label} --> {rand_sum}')\n",
        "    #rand_arr = np.array([random_no],dtype=np.uint8)\n",
        "    #sum_arr = np.array([sum_rand_label],dtype=np.uint8)\n",
        "   # print(f'{rand_arr} .....{sum_arr}')\n",
        "    #rand_arr = np.unpackbits(rand_arr)\n",
        "    #sum_arr = np.unpackbits(sum_arr)\n",
        "    #rand_arr = rand_arr[3:8]\n",
        "    #sum_arr = sum_arr[3:8]\n",
        "    #touple2 = (rand_arr.astype(dtype=np.float32),sum_arr.astype(dtype=np.float32))\n",
        "   # print(f'rand_array--> {rand_arr} sum_array--> {sum_arr} touple2 --> {type(touple2)}')\n",
        "    touple2 = (rand_input,rand_sum)\n",
        "    return self.image_dataset[index],touple2\n",
        "\n",
        "dataset = MyDataset()\n",
        "#print(f'type of dataset is {type(dataset)}')\n",
        "#hi\n",
        "batch_size = 50\n",
        "#myData_iter = iter(dataset)\n",
        "#myData = next(myData_iter)\n",
        "#print(f'Shape of data[0] and data[1]--> {type(myData[0])} ... {type(myData[1])}')\n",
        "#print(f'{myData[0]} {myData[1]}')\n",
        "#image,label = myData[0]\n",
        "#number = myData[1]\n",
        "#print(f'label,number --> {label},{number}')\n",
        "myDataLoader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
        "#print(f'length of dataset --> {len(myDataSet)} and type --> {type(myDataSet)}')\n",
        "\n",
        "class MyNetwork(nn.Module):\n",
        "  def __init__(self,batch_size):\n",
        "    super().__init__()\n",
        "    #assuming we are using Mnist data set with images 28*28\n",
        "    self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
        "    #28*28 image will convolve to 24*24\n",
        "    self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=5)\n",
        "    #28*28 image will convolve to 20*20\n",
        "    self.fc1 = nn.Linear(in_features=12*4*4,out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120,out_features=60)\n",
        "    self.out1 = nn.Linear(in_features=60,out_features=10)\n",
        "    #Add one more fully connected layer of input 10+4 and output 10+5 neurons\n",
        "    self.out2 = nn.Linear(in_features=10+10,out_features=19)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "  def forward(self,t1,t2):\n",
        "    #input to convolution operation in pytorch is batch_size,input_channel,height,width\n",
        "    x = t1\n",
        "    x = self.conv1(x) #28x28 --> 24x24\n",
        "    #visualize the learning with convolution\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x,kernel_size=2,stride=2) #half the size to 24x24 --> 12x12\n",
        "    #visualize the effect of max_pool does it good to use it now?\n",
        "    x = self.conv2(x) # 12x12 --> 8x8\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x,kernel_size=2,stride=2) #8x8 --> 4x4\n",
        "    #print(f'shape of sample data after relu --> {x.shape}')\n",
        "    x = x.reshape(self.batch_size,-1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out1(x)\n",
        "    #output one to predict image label\n",
        "    output1 = F.softmax(x,dim=1)\n",
        "    #output 2 to predict the sum, concatenate the neurons\n",
        "    #print(f'shape of x --> {x.shape} and shape of t2 --> {t2.shape}')\n",
        "    x = torch.cat([x,t2],dim=1)\n",
        "    output2 = self.out2(x)\n",
        "    output2 = F.softmax(output2,dim=1)\n",
        "    return (output1,output2)\n",
        "\n",
        "def weight_init(m):\n",
        "  if isinstance(m,nn.Linear):\n",
        "    init.uniform_(m.weight,-1,1)\n",
        "    init.uniform_(m.bias,-1,1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Available device -- > {device}')\n",
        "model = MyNetwork(batch_size)\n",
        "#schedule the model to run on GPU\n",
        "#print(f'gradient for conv1 {model.conv1.weight.grad}')\n",
        "#images,labels = next(iter(myDataLoader))\n",
        "#got the forward propogation and prediction\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "#model.apply(weight_init)\n",
        "\n",
        "model.requires_grad_(True)\n",
        "#adding epoch also now\n",
        "for epochs in range(100):\n",
        "  total_loss_mnist = 0\n",
        "  total_loss_sum = 0\n",
        "  total_correct_mnist = 0\n",
        "  total_correct_sum = 0\n",
        "  for batchs in myDataLoader:\n",
        "    #optimizer.zero_grad()\n",
        "    #print(f'type of MyDataset -- > {type(MyDataset)}')\n",
        "    #next_data = next(myData_iter)\n",
        "    images,labels_mnist = batchs[0]\n",
        "    #print(f'shape of labels_mnist is {labels_mnist.shape} and shape of image is {images.shape}')\n",
        "    #print(f'type of labels_mnist is {type(labels_mnist)} and shape of image is {type(images)}')\n",
        "    #visualization of images of Mnist dataset\n",
        "    #print(f'Shape of images -- > {images.shape}')\n",
        "    #grid = torchvision.utils.make_grid(images,nrow=10)\n",
        "    #plt.figure(figsize=[15,15])\n",
        "    #reordering image dimensions\n",
        "    #plt.imshow(grid.permute(1,2,0))\n",
        "    #labels = torch.tensor(labels_mist)\n",
        "    #print(f'type of batchs and batch[0] and batch[1]--> {type(batchs)}, {type(batchs[0])}, {type(batchs[1])}')\n",
        "    #print(f'len of batchs and batch[0] and batch[1]--> {len(batchs)}, {len(batchs[0])}, {len(batchs[1])}')\n",
        "    #print(f'{batchs[0]}, {batchs[1]}')\n",
        "    random_num,label_sum = batchs[1]\n",
        "\n",
        "    #print(f'shape of random_num is {random_num.shape} and shape of label_sum is {label_sum.shape}')\n",
        "    #print(f'type of random_num is {type(random_num)} and shape of label_sum is {type(label_sum)}')\n",
        "\n",
        "    #print(f'Shape of images --> {images.shape} and labels_mist --> {labels_mist.shape}')\n",
        "    preds_mnist,preds_sum = model(images,random_num)\n",
        "    #find the error\n",
        "    #print(f'preds_mnist --> {preds_mnist} and preds_sum --> {preds_sum} and label_sum {label_sum}')\n",
        "    loss_mnist = F.cross_entropy(preds_mnist,labels_mnist)\n",
        "    loss_sum = F.cross_entropy(preds_sum,label_sum)\n",
        "    #It does not take 2 outputs\n",
        "    #loss = F.cross_entropy((preds_mnist,preds_sum),(labels_mist,label_sum))\n",
        "    #calculate average mean loss\n",
        "    avg_loss = 0.5*loss_mnist+0.5*loss_sum\n",
        "    #calculate the gradient\n",
        "    avg_loss.backward()\n",
        "    #print(f'gradient for conv1 {model.conv1.weight.grad}')\n",
        "    #update the weights\n",
        "    optimizer.step()\n",
        "    #print(f'loss --> {loss} and values --> {pred.argmax(dim=1).eq(label).sum().item()}')\n",
        "    optimizer.zero_grad()\n",
        "    total_loss_mnist += loss_mnist\n",
        "    total_loss_sum += loss_sum\n",
        "    #print(f'Shape of pred --> {preds.shape} and labels --> {labels.shape}')\n",
        "    total_correct_mnist += preds_mnist.argmax(dim=1).eq(labels_mnist).sum().item()\n",
        "    #print(f'shape of preds_mnist {preds_mnist.shape} and shape of preds_sum {preds_sum.shape}')\n",
        "    #print(f'shape of label_sum is {label_sum.shape} and shape of lables_mist {labels_mnist.shape}')\n",
        "    total_correct_sum += preds_sum.argmax(dim=1).eq(label_sum.argmax(dim=1)).sum().item()\n",
        "    #print(f'{model.out2.state_dict()}')\n",
        "  print(f'epoch --> {epochs} loss_mnist --> {total_loss_mnist} and correct predictions --> {total_correct_mnist}')\n",
        "  print(f'epoch --> {epochs} loss_sum --> {total_loss_sum} and correct predictions of sum --> {total_correct_sum}')"
      ]
    }
  ]
}